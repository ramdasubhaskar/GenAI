# -*- coding: utf-8 -*-
"""ChatRep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1On-tnm49a8DPidqzWGB1AvDeHm0oYrDz
"""

!pip install langchain

!pip install chromadb

!pip install tiktoken

# Commented out IPython magic to ensure Python compatibility.
# %pip install beautifulsoup4 requests pandas langchain tiktoken pyarrow fastparquet chromadb typing-inspect==0.8.0 typing_extensions==4.5.0 --user

!pip install pydantic==1.10.9

# pip install --force-reinstall typing-extensions==4.5.0

!pip install pydantic -U

!pip install huggingface_hub

!pip install sentence_transformers
!pip install streamlit


import os
import streamlit as st
# os.environ["OPENAI_API_KEY"] = ""
#api_token = st.sidebar.text_input("Enter your Huggingface API token:")

os.environ["HUGGINGFACEHUB_API_TOKEN"]='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'

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.document_loaders import TextLoader
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.document_loaders import PyPDFLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

from langchain.chains import ConversationalRetrievalChain
from langchain.embeddings import HuggingFaceEmbeddings

# import streamlit as st
from langchain import HuggingFaceHub
# from apikey_hungingface import apikey_hungingface
from langchain import PromptTemplate, LLMChain
import tempfile
from PIL import Image

st.title("ü¶úÔ∏èüîó GPT Annual Report Analyzer")
#img = Image.open('ChatDoc_coverpic.jpg')


# display image using streamlit
# width is used to set the width of an image
#st.image(img)



# loader = None

# #upload your csv data file
# #uploaded_file = st.sidebar.file_uploader("upload", type="csv")
# uploaded_file = st.sidebar.file_uploader("upload", type="pdf")

# if uploaded_file :
#     with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
#         tmp_file.write(uploaded_file.getvalue())
#         tmp_file_path = tmp_file.name

#     #loader = CSVLoader(file_path=tmp_file_path, encoding="utf-8")
#     loader = PyPDFLoader(file_path=tmp_file_path)
#     # data = loader.load()

# Set up the language model using the Hugging Face Hub repository
repo_id = "tiiuae/falcon-7b-instruct"
temperature = st.sidebar.slider('Creativeness(temperature)', 0.0, 1.0, 0.3)
llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={"temperature": temperature, "max_new_tokens": 2000})


# load document
loader = PyPDFLoader("annualreport.pdf")
# loader= CSVLoader(file_path=r"C:\Users\manga\Downloads\genai\MC_PowerBI_Simulation.csv")

# loader

documents = loader.load()
print(documents)

# split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
# select which embeddings we want to use
# embeddings = OpenAIEmbeddings()
embeddings = HuggingFaceEmbeddings()
# create the vectorestore to use as the index
db = Chroma.from_documents(texts, embeddings)
# expose this index in a retriever interface
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k":2})
# create a chain to answer questions
# qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever)
qa = ConversationalRetrievalChain.from_llm(llm, retriever)
chat_history = []
query = st.text_input("Enter your input text:", value="what is the net profit contributed by operating group?")
# query = "what is the region which highest TCV?"
# result = qa({"question": query, "chat_history": chat_history})

if st.button("Generate Text"):
    with st.spinner("Generating Answer..."):
            result = qa({"question": query, "chat_history": chat_history})
    # Display the generated text
    st.write("Generated Text:")
    st.write(result)
